{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec3dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_transformers==1.0 in d:\\anaconda\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: regex in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (4.64.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (2.27.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (1.13.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (1.21.5)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (0.1.97)\n",
      "Requirement already satisfied: boto3 in d:\\anaconda\\lib\\site-packages (from pytorch_transformers==1.0) (1.21.32)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch>=0.4.1->pytorch_transformers==1.0) (4.1.1)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in d:\\anaconda\\lib\\site-packages (from boto3->pytorch_transformers==1.0) (1.24.32)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anaconda\\lib\\site-packages (from boto3->pytorch_transformers==1.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in d:\\anaconda\\lib\\site-packages (from boto3->pytorch_transformers==1.0) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in d:\\anaconda\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3->pytorch_transformers==1.0) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anaconda\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3->pytorch_transformers==1.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.32->boto3->pytorch_transformers==1.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests->pytorch_transformers==1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->pytorch_transformers==1.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->pytorch_transformers==1.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->pytorch_transformers==1.0) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_transformers==1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be6077",
   "metadata": {},
   "source": [
    "load model and use it(need to download the model to save in own computer in huggingface\n",
    "https://huggingface.co/gpt2/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75245e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\ginny\\.cache\\torch\\pytorch_transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\ginny\\.cache\\torch\\pytorch_transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load a pre-trained model of the word splitter\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode the input using GPT2Tokenizer\n",
    "text = \"Yesterday, a man named Jack said he saw an alien,\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a212ce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5915159"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/clean_shortjokes.txt', 'r') as f:\n",
    "    dataset = f.read()\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78adeb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1536628 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([25610, 60])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_text = tokenizer.encode(dataset)\n",
    "del(dataset)\n",
    "\n",
    "dataset_cut = []\n",
    "for i in range(len(indexed_text)//60):\n",
    "     # Segment the string to a length of 60\n",
    "    dataset_cut.append(indexed_text[i*60:i*60+60])\n",
    "del(indexed_text)\n",
    "\n",
    "dataset_tensor = torch.tensor(dataset_cut)\n",
    "dataset_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c741afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1747b2f8a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Build dataset and data iterator, set batch_size size to 2\n",
    "train_set = TensorDataset(dataset_tensor,\n",
    "                          dataset_tensor)  # Labels are the same as the sample data\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=2,\n",
    "                          shuffle=False)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da027a",
   "metadata": {},
   "source": [
    "Use CPU to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e7cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83cf23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file C:/Users/ginny/pythonStudy/gpt-2-master/model\\config.json\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file C:/Users/ginny/pythonStudy/gpt-2-master/model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import GPT2LMHeadModel\n",
    "model= GPT2LMHeadModel.from_pretrained(\"C:/Users/ginny/pythonStudy/gpt-2-master/model\")# Here is the address of the GPT-2 model file, I saved it on my own computer and did not pack it into the assignment folder because it was too large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade4583",
   "metadata": {},
   "source": [
    "training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "pre = time.time()\n",
    "\n",
    "epoch = 2  # Cycle study 2 times\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Define Optimizer\n",
    "\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).to(device), Variable(\n",
    "            target).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits, _ = model(data, labels=target)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx == len(train_loader)-1:\n",
    "            # Output the results at the end of each Epoch\n",
    "            print('average loss:', total_loss/len(train_loader))\n",
    "\n",
    "print('training timeï¼š', time.time()-pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56a05d",
   "metadata": {},
   "source": [
    "generate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A dog\"  # Here you can also enter different English texts\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "model.eval()\n",
    "total_predicted_text = text\n",
    "\n",
    "# Make 20 predictions from the trained model\n",
    "for _ in range(20):\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    predicted_index = select_top_k(predictions, k=10)\n",
    "\n",
    "    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "    total_predicted_text += tokenizer.decode(predicted_index)\n",
    "    if '<|endoftext|>' in total_predicted_text:\n",
    "        # If the end-of-text flag appears, end text generation\n",
    "        break\n",
    "\n",
    "    indexed_tokens += [predicted_index]\n",
    "\n",
    "    if len(indexed_tokens) > 60:\n",
    "        # Maximum model input length is 60, truncated if length is too long\n",
    "        indexed_tokens = indexed_tokens[-60:]\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "print(total_predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
